{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting bangla dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\BanglaLekha-Isolated already present - Skipping extraction of .\\BanglaLekha-Isolated.zip\n",
      "['.\\\\BanglaLekha-Isolated\\\\1', '.\\\\BanglaLekha-Isolated\\\\10', '.\\\\BanglaLekha-Isolated\\\\11', '.\\\\BanglaLekha-Isolated\\\\12', '.\\\\BanglaLekha-Isolated\\\\13', '.\\\\BanglaLekha-Isolated\\\\14', '.\\\\BanglaLekha-Isolated\\\\15', '.\\\\BanglaLekha-Isolated\\\\16', '.\\\\BanglaLekha-Isolated\\\\17', '.\\\\BanglaLekha-Isolated\\\\18', '.\\\\BanglaLekha-Isolated\\\\19', '.\\\\BanglaLekha-Isolated\\\\2', '.\\\\BanglaLekha-Isolated\\\\20', '.\\\\BanglaLekha-Isolated\\\\21', '.\\\\BanglaLekha-Isolated\\\\22', '.\\\\BanglaLekha-Isolated\\\\23', '.\\\\BanglaLekha-Isolated\\\\24', '.\\\\BanglaLekha-Isolated\\\\25', '.\\\\BanglaLekha-Isolated\\\\26', '.\\\\BanglaLekha-Isolated\\\\27', '.\\\\BanglaLekha-Isolated\\\\28', '.\\\\BanglaLekha-Isolated\\\\29', '.\\\\BanglaLekha-Isolated\\\\3', '.\\\\BanglaLekha-Isolated\\\\30', '.\\\\BanglaLekha-Isolated\\\\31', '.\\\\BanglaLekha-Isolated\\\\32', '.\\\\BanglaLekha-Isolated\\\\33', '.\\\\BanglaLekha-Isolated\\\\34', '.\\\\BanglaLekha-Isolated\\\\35', '.\\\\BanglaLekha-Isolated\\\\36', '.\\\\BanglaLekha-Isolated\\\\37', '.\\\\BanglaLekha-Isolated\\\\38', '.\\\\BanglaLekha-Isolated\\\\39', '.\\\\BanglaLekha-Isolated\\\\4', '.\\\\BanglaLekha-Isolated\\\\40', '.\\\\BanglaLekha-Isolated\\\\41', '.\\\\BanglaLekha-Isolated\\\\42', '.\\\\BanglaLekha-Isolated\\\\43', '.\\\\BanglaLekha-Isolated\\\\44', '.\\\\BanglaLekha-Isolated\\\\45', '.\\\\BanglaLekha-Isolated\\\\46', '.\\\\BanglaLekha-Isolated\\\\47', '.\\\\BanglaLekha-Isolated\\\\48', '.\\\\BanglaLekha-Isolated\\\\49', '.\\\\BanglaLekha-Isolated\\\\5', '.\\\\BanglaLekha-Isolated\\\\50', '.\\\\BanglaLekha-Isolated\\\\51', '.\\\\BanglaLekha-Isolated\\\\52', '.\\\\BanglaLekha-Isolated\\\\53', '.\\\\BanglaLekha-Isolated\\\\54', '.\\\\BanglaLekha-Isolated\\\\55', '.\\\\BanglaLekha-Isolated\\\\56', '.\\\\BanglaLekha-Isolated\\\\57', '.\\\\BanglaLekha-Isolated\\\\58', '.\\\\BanglaLekha-Isolated\\\\59', '.\\\\BanglaLekha-Isolated\\\\6', '.\\\\BanglaLekha-Isolated\\\\60', '.\\\\BanglaLekha-Isolated\\\\61', '.\\\\BanglaLekha-Isolated\\\\62', '.\\\\BanglaLekha-Isolated\\\\63', '.\\\\BanglaLekha-Isolated\\\\64', '.\\\\BanglaLekha-Isolated\\\\65', '.\\\\BanglaLekha-Isolated\\\\66', '.\\\\BanglaLekha-Isolated\\\\67', '.\\\\BanglaLekha-Isolated\\\\68', '.\\\\BanglaLekha-Isolated\\\\69', '.\\\\BanglaLekha-Isolated\\\\7', '.\\\\BanglaLekha-Isolated\\\\70', '.\\\\BanglaLekha-Isolated\\\\71', '.\\\\BanglaLekha-Isolated\\\\72', '.\\\\BanglaLekha-Isolated\\\\73', '.\\\\BanglaLekha-Isolated\\\\74', '.\\\\BanglaLekha-Isolated\\\\75', '.\\\\BanglaLekha-Isolated\\\\76', '.\\\\BanglaLekha-Isolated\\\\77', '.\\\\BanglaLekha-Isolated\\\\78', '.\\\\BanglaLekha-Isolated\\\\79', '.\\\\BanglaLekha-Isolated\\\\8', '.\\\\BanglaLekha-Isolated\\\\80', '.\\\\BanglaLekha-Isolated\\\\81', '.\\\\BanglaLekha-Isolated\\\\82', '.\\\\BanglaLekha-Isolated\\\\83', '.\\\\BanglaLekha-Isolated\\\\84', '.\\\\BanglaLekha-Isolated\\\\9']\n"
     ]
    }
   ],
   "source": [
    "## file structure- zipFileName/classes/img.class\n",
    "num_classes=84\n",
    "np.random.seed(133)\n",
    "data_root='.'\n",
    "full_filename=os.path.join(data_root, 'BanglaLekha-Isolated.zip')\n",
    "\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "    root=os.path.splitext(os.path.splitext(filename)[0])[0]\n",
    "    root=root #as the classesdata are in images folder\n",
    "    if os.path.isdir(root)and not force:\n",
    "        print('%s already present - Skipping extraction of %s' %(root, filename))\n",
    "    else:\n",
    "        print('extracting data %s.this may take a while .please wait.' %root)\n",
    "        zip=zipfile.ZipFile(filename)\n",
    "        sys.stdout.flush()\n",
    "        zip.extractall(data_root)\n",
    "        zip.close()\n",
    "    data_folders = [\n",
    "        os.path.join(root, d)for d in sorted(os.listdir(root)) #listing all the classes directory\n",
    "        if os.path.isdir(os.path.join(root, d))]\n",
    "    if len(data_folders) !=num_classes: #checks lengths\n",
    "        raise Exception(\n",
    "            'Expected %d folders , one per class. found %d instead.' % (\n",
    "                num_classes,len(data_folders)))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "\n",
    "full_folders=maybe_extract(full_filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imagepad():\n",
    "    size = 28, 28\n",
    "    root='.'\n",
    "    filename= os.path.join( root, 'BanglaLekha-Isolated') \n",
    "    data_folders = [os.path.join(filename, d)for d in sorted(os.listdir(filename))\n",
    "                    if os.path.isdir(os.path.join(filename, d))]\n",
    "    print(filename)\n",
    "    for folder in data_folders:\n",
    "        print(folder)\n",
    "        image_files=os.listdir(folder)\n",
    "        for image in image_files:\n",
    "            infile=os.path.join(folder,image)\n",
    "            print (infile)\n",
    "            #infile=\"shoro1.png\"\n",
    "            #for infile in sys.argv[1:]:\n",
    "            outfile = os.path.splitext(infile)[0]\n",
    "            try:\n",
    "                im = Image.open(infile)\n",
    "                im.thumbnail(size, Image.ANTIALIAS)\n",
    "                #im.save(\"beforePaddign1.png\")\n",
    "                background= Image.new('1',size )#1 (1-bit pixels, black and white, stored with one pixel per byte)\n",
    "                background.paste(im, (int((size[0]-im.size[0])/2), int((size[1]-im.size[1])/2)))\n",
    "                background.save(infile)\n",
    "\n",
    "\n",
    "\n",
    "            except IOError:\n",
    "                print (infile)\n",
    "                #return background\n",
    "imagepad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert images into npArray and save into pickle file with separate class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickling .\\BanglaLekha-Isolated\\1.pickle.\n",
      ".\\BanglaLekha-Isolated\\1\n",
      "Full dataset tensor: (1975L, 28L, 28L)\n",
      "mean: -0.328435\n",
      "standard deviation: 0.377002\n",
      "pickling .\\BanglaLekha-Isolated\\10.pickle.\n",
      ".\\BanglaLekha-Isolated\\10\n",
      "Full dataset tensor: (1981L, 28L, 28L)\n",
      "mean: -0.285105\n",
      "standard deviation: 0.41075\n",
      "pickling .\\BanglaLekha-Isolated\\11.pickle.\n",
      ".\\BanglaLekha-Isolated\\11\n",
      "Full dataset tensor: (1982L, 28L, 28L)\n",
      "mean: -0.344334\n",
      "standard deviation: 0.362538\n",
      "pickling .\\BanglaLekha-Isolated\\12.pickle.\n",
      ".\\BanglaLekha-Isolated\\12\n",
      "Full dataset tensor: (1984L, 28L, 28L)\n",
      "mean: -0.316436\n",
      "standard deviation: 0.387128\n",
      "pickling .\\BanglaLekha-Isolated\\13.pickle.\n",
      ".\\BanglaLekha-Isolated\\13\n",
      "Full dataset tensor: (1969L, 28L, 28L)\n",
      "mean: -0.332469\n",
      "standard deviation: 0.373449\n",
      "pickling .\\BanglaLekha-Isolated\\14.pickle.\n",
      ".\\BanglaLekha-Isolated\\14\n",
      "Full dataset tensor: (1983L, 28L, 28L)\n",
      "mean: -0.336223\n",
      "standard deviation: 0.370073\n",
      "pickling .\\BanglaLekha-Isolated\\15.pickle.\n",
      ".\\BanglaLekha-Isolated\\15\n",
      "Full dataset tensor: (1975L, 28L, 28L)\n",
      "mean: -0.314235\n",
      "standard deviation: 0.388917\n",
      "pickling .\\BanglaLekha-Isolated\\16.pickle.\n",
      ".\\BanglaLekha-Isolated\\16\n",
      "Full dataset tensor: (1980L, 28L, 28L)\n",
      "mean: -0.270061\n",
      "standard deviation: 0.420793\n",
      "pickling .\\BanglaLekha-Isolated\\17.pickle.\n",
      ".\\BanglaLekha-Isolated\\17\n",
      "Full dataset tensor: (1983L, 28L, 28L)\n",
      "mean: -0.313346\n",
      "standard deviation: 0.389634\n",
      "pickling .\\BanglaLekha-Isolated\\18.pickle.\n",
      ".\\BanglaLekha-Isolated\\18\n",
      "Full dataset tensor: (1981L, 28L, 28L)\n",
      "mean: -0.325137\n",
      "standard deviation: 0.37985\n",
      "pickling .\\BanglaLekha-Isolated\\19.pickle.\n",
      ".\\BanglaLekha-Isolated\\19\n",
      "Full dataset tensor: (1980L, 28L, 28L)\n",
      "mean: -0.314888\n",
      "standard deviation: 0.388388\n",
      "pickling .\\BanglaLekha-Isolated\\2.pickle.\n",
      ".\\BanglaLekha-Isolated\\2\n",
      "Full dataset tensor: (1980L, 28L, 28L)\n",
      "mean: -0.343805\n",
      "standard deviation: 0.36304\n",
      "pickling .\\BanglaLekha-Isolated\\20.pickle.\n",
      ".\\BanglaLekha-Isolated\\20\n",
      "Full dataset tensor: (1982L, 28L, 28L)\n",
      "mean: -0.339894\n",
      "standard deviation: 0.366704\n",
      "pickling .\\BanglaLekha-Isolated\\21.pickle.\n",
      ".\\BanglaLekha-Isolated\\21\n",
      "Full dataset tensor: (1988L, 28L, 28L)\n",
      "mean: -0.313228\n",
      "standard deviation: 0.389728\n",
      "pickling .\\BanglaLekha-Isolated\\22.pickle.\n",
      ".\\BanglaLekha-Isolated\\22\n",
      "Full dataset tensor: (1977L, 28L, 28L)\n",
      "mean: -0.368132\n",
      "standard deviation: 0.338347\n",
      "pickling .\\BanglaLekha-Isolated\\23.pickle.\n",
      ".\\BanglaLekha-Isolated\\23\n",
      "Full dataset tensor: (1941L, 28L, 28L)\n",
      "mean: -0.355872\n",
      "standard deviation: 0.35122\n",
      "pickling .\\BanglaLekha-Isolated\\24.pickle.\n",
      ".\\BanglaLekha-Isolated\\24\n",
      "Full dataset tensor: (1983L, 28L, 28L)\n",
      "mean: -0.327231\n",
      "standard deviation: 0.378047\n",
      "pickling .\\BanglaLekha-Isolated\\25.pickle.\n",
      ".\\BanglaLekha-Isolated\\25\n",
      "Full dataset tensor: (1977L, 28L, 28L)\n",
      "mean: -0.315281\n",
      "standard deviation: 0.388069\n",
      "pickling .\\BanglaLekha-Isolated\\26.pickle.\n",
      ".\\BanglaLekha-Isolated\\26\n",
      "Full dataset tensor: (1980L, 28L, 28L)\n",
      "mean: -0.327884\n",
      "standard deviation: 0.377481\n",
      "pickling .\\BanglaLekha-Isolated\\27.pickle.\n",
      ".\\BanglaLekha-Isolated\\27\n",
      "Full dataset tensor: (1975L, 28L, 28L)\n",
      "mean: -0.31873\n",
      "standard deviation: 0.385242\n",
      "pickling .\\BanglaLekha-Isolated\\28.pickle.\n",
      ".\\BanglaLekha-Isolated\\28\n",
      "Full dataset tensor: (1977L, 28L, 28L)\n",
      "mean: -0.330123\n",
      "standard deviation: 0.375525\n",
      "pickling .\\BanglaLekha-Isolated\\29.pickle.\n",
      ".\\BanglaLekha-Isolated\\29\n",
      "Full dataset tensor: (1981L, 28L, 28L)\n",
      "mean: -0.336236\n",
      "standard deviation: 0.370062\n",
      "pickling .\\BanglaLekha-Isolated\\3.pickle.\n",
      ".\\BanglaLekha-Isolated\\3\n",
      "Full dataset tensor: (1979L, 28L, 28L)\n",
      "mean: -0.361461\n",
      "standard deviation: 0.345465\n",
      "pickling .\\BanglaLekha-Isolated\\30.pickle.\n",
      ".\\BanglaLekha-Isolated\\30\n",
      "Full dataset tensor: (1968L, 28L, 28L)\n",
      "mean: -0.316069\n",
      "standard deviation: 0.387428\n",
      "pickling .\\BanglaLekha-Isolated\\31.pickle.\n",
      ".\\BanglaLekha-Isolated\\31\n",
      "Full dataset tensor: (1983L, 28L, 28L)\n",
      "mean: -0.300915\n",
      "standard deviation: 0.399312\n",
      "pickling .\\BanglaLekha-Isolated\\32.pickle.\n",
      ".\\BanglaLekha-Isolated\\32\n",
      "Full dataset tensor: (1985L, 28L, 28L)\n",
      "mean: -0.327716\n",
      "standard deviation: 0.377627\n",
      "pickling .\\BanglaLekha-Isolated\\33.pickle.\n",
      ".\\BanglaLekha-Isolated\\33\n",
      "Full dataset tensor: (1982L, 28L, 28L)\n",
      "mean: -0.320717\n",
      "standard deviation: 0.383589\n",
      "pickling .\\BanglaLekha-Isolated\\34.pickle.\n",
      ".\\BanglaLekha-Isolated\\34\n",
      "Full dataset tensor: (1983L, 28L, 28L)\n",
      "mean: -0.310206\n",
      "standard deviation: 0.392138\n",
      "pickling .\\BanglaLekha-Isolated\\35.pickle.\n",
      ".\\BanglaLekha-Isolated\\35\n",
      "Full dataset tensor: (1980L, 28L, 28L)\n",
      "mean: -0.311055\n",
      "standard deviation: 0.391465\n",
      "pickling .\\BanglaLekha-Isolated\\36.pickle.\n",
      ".\\BanglaLekha-Isolated\\36\n",
      "Full dataset tensor: (1983L, 28L, 28L)\n",
      "mean: -0.331509\n",
      "standard deviation: 0.374302\n",
      "pickling .\\BanglaLekha-Isolated\\37.pickle.\n",
      ".\\BanglaLekha-Isolated\\37\n",
      "Full dataset tensor: (1985L, 28L, 28L)\n",
      "mean: -0.322424\n",
      "standard deviation: 0.382155\n",
      "pickling .\\BanglaLekha-Isolated\\38.pickle.\n",
      ".\\BanglaLekha-Isolated\\38\n",
      "Full dataset tensor: (1984L, 28L, 28L)\n",
      "mean: -0.32211\n",
      "standard deviation: 0.38242\n",
      "pickling .\\BanglaLekha-Isolated\\39.pickle.\n",
      ".\\BanglaLekha-Isolated\\39\n",
      "Full dataset tensor: (1969L, 28L, 28L)\n",
      "mean: -0.317925\n",
      "standard deviation: 0.385907\n",
      "pickling .\\BanglaLekha-Isolated\\4.pickle.\n",
      ".\\BanglaLekha-Isolated\\4\n",
      "Full dataset tensor: (1982L, 28L, 28L)\n",
      "mean: -0.35016\n",
      "standard deviation: 0.356914\n",
      "pickling .\\BanglaLekha-Isolated\\40.pickle.\n",
      ".\\BanglaLekha-Isolated\\40\n",
      "Full dataset tensor: (1979L, 28L, 28L)\n",
      "mean: -0.333471\n",
      "standard deviation: 0.372555\n",
      "pickling .\\BanglaLekha-Isolated\\41.pickle.\n",
      ".\\BanglaLekha-Isolated\\41\n",
      "Full dataset tensor: (1972L, 28L, 28L)\n",
      "mean: -0.310578\n",
      "standard deviation: 0.391843\n",
      "pickling .\\BanglaLekha-Isolated\\42.pickle.\n",
      ".\\BanglaLekha-Isolated\\42\n",
      "Full dataset tensor: (1981L, 28L, 28L)\n",
      "mean: -0.349011\n",
      "standard deviation: 0.358038\n",
      "pickling .\\BanglaLekha-Isolated\\43.pickle.\n",
      ".\\BanglaLekha-Isolated\\43\n",
      "Full dataset tensor: (1979L, 28L, 28L)\n",
      "mean: -0.319329\n",
      "standard deviation: 0.384746\n",
      "pickling .\\BanglaLekha-Isolated\\44.pickle.\n",
      ".\\BanglaLekha-Isolated\\44\n",
      "Full dataset tensor: (1984L, 28L, 28L)\n",
      "mean: -0.339765\n",
      "standard deviation: 0.366824\n",
      "pickling .\\BanglaLekha-Isolated\\45.pickle.\n",
      ".\\BanglaLekha-Isolated\\45\n",
      "Full dataset tensor: (1969L, 28L, 28L)\n",
      "mean: -0.354463\n",
      "standard deviation: 0.352642\n",
      "pickling .\\BanglaLekha-Isolated\\46.pickle.\n",
      ".\\BanglaLekha-Isolated\\46\n",
      "Full dataset tensor: (1980L, 28L, 28L)\n",
      "mean: -0.333154\n",
      "standard deviation: 0.372839\n",
      "pickling .\\BanglaLekha-Isolated\\47.pickle.\n",
      ".\\BanglaLekha-Isolated\\47\n",
      "Full dataset tensor: (1980L, 28L, 28L)\n",
      "mean: -0.351286\n",
      "standard deviation: 0.355806\n",
      "pickling .\\BanglaLekha-Isolated\\48.pickle.\n",
      ".\\BanglaLekha-Isolated\\48\n",
      "Full dataset tensor: (1983L, 28L, 28L)\n",
      "mean: -0.373421\n",
      "standard deviation: 0.332501\n",
      "pickling .\\BanglaLekha-Isolated\\49.pickle.\n",
      ".\\BanglaLekha-Isolated\\49\n",
      "Full dataset tensor: (1981L, 28L, 28L)\n",
      "mean: -0.384208\n",
      "standard deviation: 0.319975\n",
      "pickling .\\BanglaLekha-Isolated\\5.pickle.\n",
      ".\\BanglaLekha-Isolated\\5\n",
      "Full dataset tensor: (1982L, 28L, 28L)\n",
      "mean: -0.349086\n",
      "standard deviation: 0.357965\n",
      "pickling .\\BanglaLekha-Isolated\\50.pickle.\n",
      ".\\BanglaLekha-Isolated\\50\n",
      "Full dataset tensor: (1981L, 28L, 28L)\n",
      "mean: -0.308635\n",
      "standard deviation: 0.393375\n",
      "pickling .\\BanglaLekha-Isolated\\51.pickle.\n",
      ".\\BanglaLekha-Isolated\\51\n",
      "Full dataset tensor: (1982L, 28L, 28L)\n",
      "mean: -0.215219\n",
      "standard deviation: 0.45131\n",
      "pickling .\\BanglaLekha-Isolated\\52.pickle.\n",
      ".\\BanglaLekha-Isolated\\52\n",
      "Full dataset tensor: (1982L, 28L, 28L)\n",
      "mean: -0.27786\n",
      "standard deviation: 0.415685\n",
      "pickling .\\BanglaLekha-Isolated\\53.pickle.\n",
      ".\\BanglaLekha-Isolated\\53\n",
      "Full dataset tensor: (1953L, 28L, 28L)\n",
      "mean: -0.350259\n",
      "standard deviation: 0.356817\n",
      "pickling .\\BanglaLekha-Isolated\\54.pickle.\n",
      ".\\BanglaLekha-Isolated\\54\n",
      "Full dataset tensor: (1975L, 28L, 28L)\n",
      "mean: -0.286974\n",
      "standard deviation: 0.409446\n",
      "pickling .\\BanglaLekha-Isolated\\55.pickle.\n",
      ".\\BanglaLekha-Isolated\\55\n",
      "Full dataset tensor: (1980L, 28L, 28L)\n",
      "mean: -0.274527\n",
      "standard deviation: 0.417893\n",
      "pickling .\\BanglaLekha-Isolated\\56.pickle.\n",
      ".\\BanglaLekha-Isolated\\56\n",
      "Full dataset tensor: (1986L, 28L, 28L)\n",
      "mean: -0.245985\n",
      "standard deviation: 0.435306\n",
      "pickling .\\BanglaLekha-Isolated\\57.pickle.\n",
      ".\\BanglaLekha-Isolated\\57\n",
      "Full dataset tensor: (1981L, 28L, 28L)\n",
      "mean: -0.306041\n",
      "standard deviation: 0.395397\n",
      "pickling .\\BanglaLekha-Isolated\\58.pickle.\n",
      ".\\BanglaLekha-Isolated\\58\n",
      "Full dataset tensor: (1958L, 28L, 28L)\n",
      "mean: -0.334223\n",
      "standard deviation: 0.371881\n",
      "pickling .\\BanglaLekha-Isolated\\59.pickle.\n",
      ".\\BanglaLekha-Isolated\\59\n",
      "Full dataset tensor: (1984L, 28L, 28L)\n",
      "mean: -0.323223\n",
      "standard deviation: 0.38148\n",
      "pickling .\\BanglaLekha-Isolated\\6.pickle.\n",
      ".\\BanglaLekha-Isolated\\6\n",
      "Full dataset tensor: (1982L, 28L, 28L)\n",
      "mean: -0.333126\n",
      "standard deviation: 0.372864\n",
      "pickling .\\BanglaLekha-Isolated\\60.pickle.\n",
      ".\\BanglaLekha-Isolated\\60\n",
      "Full dataset tensor: (1967L, 28L, 28L)\n",
      "mean: -0.314152\n",
      "standard deviation: 0.388984\n",
      "pickling .\\BanglaLekha-Isolated\\61.pickle.\n",
      ".\\BanglaLekha-Isolated\\61\n",
      "Full dataset tensor: (1981L, 28L, 28L)\n",
      "mean: -0.285097\n",
      "standard deviation: 0.410755\n",
      "pickling .\\BanglaLekha-Isolated\\62.pickle.\n",
      ".\\BanglaLekha-Isolated\\62\n",
      "Full dataset tensor: (1980L, 28L, 28L)\n",
      "mean: -0.323088\n",
      "standard deviation: 0.381594\n",
      "pickling .\\BanglaLekha-Isolated\\63.pickle.\n",
      ".\\BanglaLekha-Isolated\\63\n",
      "Full dataset tensor: (1953L, 28L, 28L)\n",
      "mean: -0.29917\n",
      "standard deviation: 0.400621\n",
      "pickling .\\BanglaLekha-Isolated\\64.pickle.\n",
      ".\\BanglaLekha-Isolated\\64\n",
      "Full dataset tensor: (1985L, 28L, 28L)\n",
      "mean: -0.301622\n",
      "standard deviation: 0.398778\n",
      "pickling .\\BanglaLekha-Isolated\\65.pickle.\n",
      ".\\BanglaLekha-Isolated\\65\n",
      "Full dataset tensor: (1978L, 28L, 28L)\n",
      "mean: -0.327194\n",
      "standard deviation: 0.378079\n",
      "pickling .\\BanglaLekha-Isolated\\66.pickle.\n",
      ".\\BanglaLekha-Isolated\\66\n",
      "Full dataset tensor: (1978L, 28L, 28L)\n",
      "mean: -0.327839\n",
      "standard deviation: 0.37752\n",
      "pickling .\\BanglaLekha-Isolated\\67.pickle.\n",
      ".\\BanglaLekha-Isolated\\67\n",
      "Full dataset tensor: (1983L, 28L, 28L)\n",
      "mean: -0.313439\n",
      "standard deviation: 0.389559\n",
      "pickling .\\BanglaLekha-Isolated\\68.pickle.\n",
      ".\\BanglaLekha-Isolated\\68\n",
      "Full dataset tensor: (1982L, 28L, 28L)\n",
      "mean: -0.311307\n",
      "standard deviation: 0.391265\n",
      "pickling .\\BanglaLekha-Isolated\\69.pickle.\n",
      ".\\BanglaLekha-Isolated\\69\n",
      "Full dataset tensor: (1985L, 28L, 28L)\n",
      "mean: -0.301703\n",
      "standard deviation: 0.398717\n",
      "pickling .\\BanglaLekha-Isolated\\7.pickle.\n",
      ".\\BanglaLekha-Isolated\\7\n",
      "Full dataset tensor: (1971L, 28L, 28L)\n",
      "mean: -0.306962\n",
      "standard deviation: 0.394683\n",
      "pickling .\\BanglaLekha-Isolated\\70.pickle.\n",
      ".\\BanglaLekha-Isolated\\70\n",
      "Full dataset tensor: (1984L, 28L, 28L)\n",
      "mean: -0.299997\n",
      "standard deviation: 0.400002\n",
      "pickling .\\BanglaLekha-Isolated\\71.pickle.\n",
      ".\\BanglaLekha-Isolated\\71\n",
      "Full dataset tensor: (1966L, 28L, 28L)\n",
      "mean: -0.338856\n",
      "standard deviation: 0.367664\n",
      "pickling .\\BanglaLekha-Isolated\\72.pickle.\n",
      ".\\BanglaLekha-Isolated\\72\n",
      "Full dataset tensor: (1968L, 28L, 28L)\n",
      "mean: -0.288285\n",
      "standard deviation: 0.408524\n",
      "pickling .\\BanglaLekha-Isolated\\73.pickle.\n",
      ".\\BanglaLekha-Isolated\\73\n",
      "Full dataset tensor: (1985L, 28L, 28L)\n",
      "mean: -0.291726\n",
      "standard deviation: 0.406074\n",
      "pickling .\\BanglaLekha-Isolated\\74.pickle.\n",
      ".\\BanglaLekha-Isolated\\74\n",
      "Full dataset tensor: (1984L, 28L, 28L)\n",
      "mean: -0.297619\n",
      "standard deviation: 0.401775\n",
      "pickling .\\BanglaLekha-Isolated\\75.pickle.\n",
      ".\\BanglaLekha-Isolated\\75\n",
      "Full dataset tensor: (1965L, 28L, 28L)\n",
      "mean: -0.277484\n",
      "standard deviation: 0.415936\n",
      "pickling .\\BanglaLekha-Isolated\\76.pickle.\n",
      ".\\BanglaLekha-Isolated\\76\n",
      "Full dataset tensor: (1982L, 28L, 28L)\n",
      "mean: -0.307461\n",
      "standard deviation: 0.394294\n",
      "pickling .\\BanglaLekha-Isolated\\77.pickle.\n",
      ".\\BanglaLekha-Isolated\\77\n",
      "Full dataset tensor: (1940L, 28L, 28L)\n",
      "mean: -0.341937\n",
      "standard deviation: 0.3648\n",
      "pickling .\\BanglaLekha-Isolated\\78.pickle.\n",
      ".\\BanglaLekha-Isolated\\78\n",
      "Full dataset tensor: (1981L, 28L, 28L)\n",
      "mean: -0.318282\n",
      "standard deviation: 0.385612\n",
      "pickling .\\BanglaLekha-Isolated\\79.pickle.\n",
      ".\\BanglaLekha-Isolated\\79\n",
      "Full dataset tensor: (1977L, 28L, 28L)\n",
      "mean: -0.31051\n",
      "standard deviation: 0.391898\n",
      "pickling .\\BanglaLekha-Isolated\\8.pickle.\n",
      ".\\BanglaLekha-Isolated\\8\n",
      "Full dataset tensor: (1985L, 28L, 28L)\n",
      "mean: -0.295821\n",
      "standard deviation: 0.403101\n",
      "pickling .\\BanglaLekha-Isolated\\80.pickle.\n",
      ".\\BanglaLekha-Isolated\\80\n",
      "Full dataset tensor: (1980L, 28L, 28L)\n",
      "mean: -0.340063\n",
      "standard deviation: 0.366547\n",
      "pickling .\\BanglaLekha-Isolated\\81.pickle.\n",
      ".\\BanglaLekha-Isolated\\81\n",
      "Full dataset tensor: (1975L, 28L, 28L)\n",
      "mean: -0.320489\n",
      "standard deviation: 0.38378\n",
      "pickling .\\BanglaLekha-Isolated\\82.pickle.\n",
      ".\\BanglaLekha-Isolated\\82\n",
      "Full dataset tensor: (1978L, 28L, 28L)\n",
      "mean: -0.287343\n",
      "standard deviation: 0.409187\n",
      "pickling .\\BanglaLekha-Isolated\\83.pickle.\n",
      ".\\BanglaLekha-Isolated\\83\n",
      "Full dataset tensor: (1971L, 28L, 28L)\n",
      "mean: -0.300789\n",
      "standard deviation: 0.399407\n",
      "pickling .\\BanglaLekha-Isolated\\84.pickle.\n",
      ".\\BanglaLekha-Isolated\\84\n",
      "Full dataset tensor: (1966L, 28L, 28L)\n",
      "mean: -0.327815\n",
      "standard deviation: 0.377541\n",
      "pickling .\\BanglaLekha-Isolated\\9.pickle.\n",
      ".\\BanglaLekha-Isolated\\9\n",
      "Full dataset tensor: (1984L, 28L, 28L)\n",
      "mean: -0.347098\n",
      "standard deviation: 0.359893\n"
     ]
    }
   ],
   "source": [
    "image_size=28 #pixel width and height\n",
    "pixel_depth=255.0 #Nurmber of levels per pixel\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \"\"\"Load the data for a single letter label\"\"\"\n",
    "    image_files=os.listdir(folder)\n",
    "    dataset=np.ndarray(shape=(len(image_files),image_size, image_size),dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images=0\n",
    "    for image in image_files:\n",
    "        image_file=os.path.join(folder,image)\n",
    "        try:\n",
    "            image_data=(ndimage.imread(image_file).astype(float) - pixel_depth /2)/pixel_depth\n",
    "            if image_data.shape !=(image_size,image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape),(image_file))\n",
    "            dataset[num_images,:,:]=image_data\n",
    "            num_images=num_images + 1\n",
    "        except IOError as e:\n",
    "            print('couldnot read :', image_file , ':', e , '-it\\'s ok , skipping.')\n",
    "    dataset=dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('many fewer images then expected: %d < %d' % (num_images, min_num_images))\n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('mean:', np.mean(dataset))\n",
    "    print('standard deviation:', np.std(dataset))\n",
    "    return dataset\n",
    "\n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    dataset_names=[]\n",
    "    for folder in data_folders:\n",
    "        set_filename=folder+ '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename)and not force:\n",
    "            print('%s already present - skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            print('pickling %s.' % set_filename)\n",
    "            dataset=load_letter(folder,min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb')as f:\n",
    "                    pickle.dump(dataset,f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':',e)\n",
    "    return dataset_names\n",
    "full_datasets=maybe_pickle(full_folders, 1900)#Not MNIST Large\n",
    "\n",
    "#print (full_datasets.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1975L, 28L, 28L)\n",
      "1548400\n",
      "[[-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "   0.5  0.5  0.5  0.5  0.5  0.5  0.5 -0.5  0.5 -0.5  0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5 -0.5  0.5  0.5  0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "   0.5  0.5 -0.5  0.5  0.5  0.5 -0.5 -0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "   0.5 -0.5 -0.5 -0.5 -0.5  0.5 -0.5 -0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "   0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5  0.5 -0.5 -0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5  0.5\n",
      "  -0.5 -0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5\n",
      "   0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlOGXivp/hD0FpYkG18AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x954be48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.image as mpimg\n",
    "read_pickle = pickle.load( open( \"BanglaLekha-Isolated/1.pickle\", \"rb\" ) )\n",
    "print (read_pickle.shape)\n",
    "print (read_pickle.size)\n",
    "#print (read_pickle)\n",
    "print (read_pickle[0])\n",
    "imgplot = plt.imshow(read_pickle[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " # shuffle images from each class to have random validation and training set then merge each classes into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:  (139000L, 28L, 28L) (139000L,)\n",
      "validation : (10000L, 28L, 28L) (10000L,)\n",
      "test : (10000L, 28L, 28L) (10000L,)\n"
     ]
    }
   ],
   "source": [
    "#print (train_dataset.shape)\n",
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset=np.ndarray((nb_rows, img_size , img_size), dtype=np.float32)\n",
    "        labels=np.ndarray(nb_rows,dtype=np.float32)\n",
    "    else:\n",
    "        dataset,labels=None, None\n",
    "    return dataset,labels\n",
    "def merge_datasets(pickle_files,train_size, valid_size=0):\n",
    "    num_classes= len(pickle_files)\n",
    "    valid_dataset, valid_labels= make_arrays(valid_size, image_size)#for validation \n",
    "    train_dataset, train_labels= make_arrays(train_size, image_size)\n",
    "    vsize_per_class= valid_size // num_classes\n",
    "    tsize_per_class= train_size // num_classes\n",
    "    \n",
    "    start_v,start_t =0 , 0\n",
    "    end_v, end_t =vsize_per_class , tsize_per_class\n",
    "    end_l=vsize_per_class+tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):#label=value or index of pickle files,example A,B,C,D\n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set=pickle.load(f)\n",
    "                #shuffle the letters to have random validation and training set\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter=letter_set[:vsize_per_class, : , :]\n",
    "                    valid_dataset[start_v:end_v, : , :] = valid_letter\n",
    "                    valid_labels[start_v:end_v]= label\n",
    "                    start_v +=vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "                    \n",
    "                train_letter= letter_set[vsize_per_class:end_l, :, :]\n",
    "                train_dataset[start_t:end_t, : ,:]=train_letter\n",
    "                train_labels[start_t:end_t]=label\n",
    "                start_t +=tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('unable to process data from', pickle_file, '.', e)\n",
    "            raise\n",
    "    return valid_dataset,valid_labels,train_dataset, train_labels\n",
    "\n",
    "train_size= 139000\n",
    "valid_size=10000\n",
    "test_size=10000\n",
    "\n",
    "valid_dataset, valid_labels,train_dataset,train_labels = merge_datasets(full_datasets, train_size , valid_size)\n",
    "test_dataset, test_labels,train_dataset,train_labels=merge_datasets(full_datasets,train_size,test_size)\n",
    "\n",
    "print('training: ' ,train_dataset.shape , train_labels.shape)\n",
    "print('validation :' , valid_dataset.shape , valid_labels.shape)\n",
    "print('test :', test_dataset.shape , test_labels.shape)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139000\n",
      "[ 69211 117112  77560 ..., 102436  95756 137937]\n",
      "10000\n",
      "[3946  293  591 ..., 3664 1936 9171]\n",
      "10000\n",
      "[4037 2544 9698 ..., 8874 8086 2204]\n"
     ]
    }
   ],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    print (labels.shape[0])\n",
    "    print (permutation)\n",
    "    shuffled_dataset= dataset[permutation, : , :]\n",
    "    shuffled_labels= labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "train_dataset , train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset , test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset , valid_labels = randomize(valid_dataset, valid_labels)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUlGGXyrK8EtFGX6pKMMvFfV/aaHuioFOePgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x962c160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgplot = plt.imshow(train_dataset [0])\n",
    "print (train_labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'banglaIsolated.pickle')\n",
    "try:\n",
    "    f = open(pickle_file , 'wb')\n",
    "    save= {\n",
    "        'train_dataset' : train_dataset,\n",
    "        'train_labels'  : train_labels,\n",
    "        'valid_dataset' : valid_dataset,\n",
    "        'valid_labels'  : valid_labels,\n",
    "        'test_dataset'  : test_dataset,\n",
    "        'test_labels'    : test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f , pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('unable to save data to', pickle_file , ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressed pickcle size: 499260416\n"
     ]
    }
   ],
   "source": [
    "statinfo= os.stat(pickle_file)\n",
    "print('compressed pickcle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "\n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "\n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "\n",
      " ..., \n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5  0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "\n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "\n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]]\n",
      "Time: 1.30s\n",
      "valid -> train overlap: 8368 samples\n",
      "test  -> train overlap: 89 samples\n",
      "test  -> valid overlap: 586 samples\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import hashlib\n",
    "\n",
    "f=open(pickle_file, 'rb')\n",
    "read_pickle = pickle.load( f )\n",
    "train_dataset=read_pickle['train_dataset']\n",
    "print(train_dataset)\n",
    "train_labels=read_pickle['train_labels']\n",
    "valid_dataset=read_pickle['valid_dataset']\n",
    "valid_labels=read_pickle['valid_labels']\n",
    "test_dataset=read_pickle['test_dataset']\n",
    "test_labels=read_pickle['test_labels']\n",
    "\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "train_hashes = [hashlib.sha1(x).digest() for x in train_dataset]\n",
    "valid_hashes = [hashlib.sha1(x).digest() for x in valid_dataset]\n",
    "test_hashes  = [hashlib.sha1(x).digest() for x in test_dataset]\n",
    "\n",
    "valid_in_train = np.in1d(valid_hashes, train_hashes)\n",
    "test_in_train  = np.in1d(test_hashes,  train_hashes)\n",
    "test_in_valid  = np.in1d(test_hashes,  valid_hashes)\n",
    "\n",
    "valid_keep = ~valid_in_train\n",
    "test_keep  = ~(test_in_train | test_in_valid)\n",
    "\n",
    "valid_dataset_clean = valid_dataset[valid_keep]\n",
    "valid_labels_clean  = valid_labels [valid_keep]\n",
    "\n",
    "test_dataset_clean = test_dataset[test_keep]\n",
    "test_labels_clean  = test_labels [test_keep]\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Time: %0.2fs\" % (t2 - t1))\n",
    "print(\"valid -> train overlap: %d samples\" % valid_in_train.sum())\n",
    "print(\"test  -> train overlap: %d samples\" % test_in_train.sum())\n",
    "print(\"test  -> valid overlap: %d samples\" % test_in_valid.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9341L, 28L, 28L)\n",
      "7323344\n",
      "(1632L, 28L, 28L)\n",
      "1279488\n",
      "(139000L, 28L, 28L)\n",
      "108976000\n"
     ]
    }
   ],
   "source": [
    "print (test_dataset_clean.shape)\n",
    "print (test_dataset_clean.size)\n",
    "print (valid_dataset_clean.shape)\n",
    "print (valid_dataset_clean.size)\n",
    "print (train_dataset.shape)\n",
    "print (train_dataset.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving cleaned Dataset as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'banglaIsolated_clean.pickle')\n",
    "try:\n",
    "    f = open(pickle_file , 'wb')\n",
    "    save= {\n",
    "        'train_dataset' : train_dataset,\n",
    "        'train_labels'  : train_labels,\n",
    "        'valid_dataset_clean' : valid_dataset,\n",
    "        'valid_labels'  : valid_labels,\n",
    "        'test_dataset_clean'  : test_dataset,\n",
    "        'test_labels'    : test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f , pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('unable to save data to', pickle_file , ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressed pickcle size: 499260428\n"
     ]
    }
   ],
   "source": [
    "statinfo= os.stat(pickle_file)\n",
    "print('compressed pickcle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
