{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting bangla dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\BanglaLekha_mini already present - Skipping extraction of .\\BanglaLekha_mini.zip\n",
      "['.\\\\BanglaLekha_mini\\\\1', '.\\\\BanglaLekha_mini\\\\10', '.\\\\BanglaLekha_mini\\\\2', '.\\\\BanglaLekha_mini\\\\3', '.\\\\BanglaLekha_mini\\\\4', '.\\\\BanglaLekha_mini\\\\5', '.\\\\BanglaLekha_mini\\\\6', '.\\\\BanglaLekha_mini\\\\7', '.\\\\BanglaLekha_mini\\\\8', '.\\\\BanglaLekha_mini\\\\9']\n"
     ]
    }
   ],
   "source": [
    "## file structure- zipFileName/classes/img.class\n",
    "num_classes=10\n",
    "np.random.seed(133)\n",
    "data_root='.'\n",
    "full_filename=os.path.join(data_root, 'BanglaLekha_mini.zip')\n",
    "\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "    root=os.path.splitext(os.path.splitext(filename)[0])[0]\n",
    "    root=root #as the classesdata are in images folder\n",
    "    if os.path.isdir(root)and not force:\n",
    "        print('%s already present - Skipping extraction of %s' %(root, filename))\n",
    "    else:\n",
    "        print('extracting data %s.this may take a while .please wait.' %root)\n",
    "        zip=zipfile.ZipFile(filename)\n",
    "        sys.stdout.flush()\n",
    "        zip.extractall(data_root)\n",
    "        zip.close()\n",
    "    data_folders = [\n",
    "        os.path.join(root, d)for d in sorted(os.listdir(root)) #listing all the classes directory\n",
    "        if os.path.isdir(os.path.join(root, d))]\n",
    "    if len(data_folders) !=num_classes: #checks lengths\n",
    "        raise Exception(\n",
    "            'Expected %d folders , one per class. found %d instead.' % (\n",
    "                num_classes,len(data_folders)))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "\n",
    "full_folders=maybe_extract(full_filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagepad():\n",
    "    size = 28, 28\n",
    "    root='.'\n",
    "    filename= os.path.join( root, 'BanglaLekha_mini') \n",
    "    data_folders = [os.path.join(filename, d)for d in sorted(os.listdir(filename))\n",
    "                    if os.path.isdir(os.path.join(filename, d))]\n",
    "    print(filename)\n",
    "    for folder in data_folders:\n",
    "        print(folder)\n",
    "        image_files=os.listdir(folder)\n",
    "        for image in image_files:\n",
    "            infile=os.path.join(folder,image)\n",
    "            print (infile)\n",
    "            #infile=\"shoro1.png\"\n",
    "            #for infile in sys.argv[1:]:\n",
    "            outfile = os.path.splitext(infile)[0]\n",
    "            try:\n",
    "                im = Image.open(infile)\n",
    "                im.thumbnail(size, Image.ANTIALIAS)\n",
    "                #im.save(\"beforePaddign1.png\")\n",
    "                background= Image.new('1',size )#1 (1-bit pixels, black and white, stored with one pixel per byte)\n",
    "                background.paste(im, (int((size[0]-im.size[0])/2), int((size[1]-im.size[1])/2)))\n",
    "                background.save(infile)\n",
    "\n",
    "\n",
    "\n",
    "            except IOError:\n",
    "                print (infile)\n",
    "                #return background\n",
    "imagepad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert images into npArray and save into pickle file with separate class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickling .\\BanglaLekha_mini\\1.pickle.\n",
      ".\\BanglaLekha_mini\\1\n",
      "Full dataset tensor: (1975L, 28L, 28L)\n",
      "mean: -0.328435\n",
      "standard deviation: 0.377002\n",
      "pickling .\\BanglaLekha_mini\\10.pickle.\n",
      ".\\BanglaLekha_mini\\10\n",
      "Full dataset tensor: (1981L, 28L, 28L)\n",
      "mean: -0.285105\n",
      "standard deviation: 0.41075\n",
      "pickling .\\BanglaLekha_mini\\2.pickle.\n",
      ".\\BanglaLekha_mini\\2\n",
      "Full dataset tensor: (1980L, 28L, 28L)\n",
      "mean: -0.343805\n",
      "standard deviation: 0.36304\n",
      "pickling .\\BanglaLekha_mini\\3.pickle.\n",
      ".\\BanglaLekha_mini\\3\n",
      "Full dataset tensor: (1979L, 28L, 28L)\n",
      "mean: -0.361461\n",
      "standard deviation: 0.345465\n",
      "pickling .\\BanglaLekha_mini\\4.pickle.\n",
      ".\\BanglaLekha_mini\\4\n",
      "Full dataset tensor: (1982L, 28L, 28L)\n",
      "mean: -0.35016\n",
      "standard deviation: 0.356914\n",
      "pickling .\\BanglaLekha_mini\\5.pickle.\n",
      ".\\BanglaLekha_mini\\5\n",
      "Full dataset tensor: (1982L, 28L, 28L)\n",
      "mean: -0.349086\n",
      "standard deviation: 0.357965\n",
      "pickling .\\BanglaLekha_mini\\6.pickle.\n",
      ".\\BanglaLekha_mini\\6\n",
      "Full dataset tensor: (1982L, 28L, 28L)\n",
      "mean: -0.333126\n",
      "standard deviation: 0.372864\n",
      "pickling .\\BanglaLekha_mini\\7.pickle.\n",
      ".\\BanglaLekha_mini\\7\n",
      "Full dataset tensor: (1971L, 28L, 28L)\n",
      "mean: -0.306962\n",
      "standard deviation: 0.394683\n",
      "pickling .\\BanglaLekha_mini\\8.pickle.\n",
      ".\\BanglaLekha_mini\\8\n",
      "Full dataset tensor: (1985L, 28L, 28L)\n",
      "mean: -0.295821\n",
      "standard deviation: 0.403101\n",
      "pickling .\\BanglaLekha_mini\\9.pickle.\n",
      ".\\BanglaLekha_mini\\9\n",
      "Full dataset tensor: (1984L, 28L, 28L)\n",
      "mean: -0.347098\n",
      "standard deviation: 0.359893\n"
     ]
    }
   ],
   "source": [
    "image_size=28 #pixel width and height\n",
    "pixel_depth=255.0 #Nurmber of levels per pixel\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \"\"\"Load the data for a single letter label\"\"\"\n",
    "    image_files=os.listdir(folder)\n",
    "    dataset=np.ndarray(shape=(len(image_files),image_size, image_size),dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images=0\n",
    "    for image in image_files:\n",
    "        image_file=os.path.join(folder,image)\n",
    "        try:\n",
    "            image_data=(ndimage.imread(image_file).astype(float) - pixel_depth /2)/pixel_depth\n",
    "            if image_data.shape !=(image_size,image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape),(image_file))\n",
    "            dataset[num_images,:,:]=image_data\n",
    "            num_images=num_images + 1\n",
    "        except IOError as e:\n",
    "            print('couldnot read :', image_file , ':', e , '-it\\'s ok , skipping.')\n",
    "    dataset=dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('many fewer images then expected: %d < %d' % (num_images, min_num_images))\n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('mean:', np.mean(dataset))\n",
    "    print('standard deviation:', np.std(dataset))\n",
    "    return dataset\n",
    "\n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    dataset_names=[]\n",
    "    for folder in data_folders:\n",
    "        set_filename=folder+ '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename)and not force:\n",
    "            print('%s already present - skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            print('pickling %s.' % set_filename)\n",
    "            dataset=load_letter(folder,min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb')as f:\n",
    "                    pickle.dump(dataset,f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':',e)\n",
    "    return dataset_names\n",
    "full_datasets=maybe_pickle(full_folders, 1970)#Not MNIST Large\n",
    "\n",
    "#print (full_datasets.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1975, 28, 28)\n",
      "1548400\n",
      "[[-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [ 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "   0.5  0.5  0.5  0.5  0.5  0.5  0.5 -0.5  0.5 -0.5  0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5 -0.5  0.5  0.5  0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "   0.5  0.5 -0.5  0.5  0.5  0.5 -0.5 -0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "   0.5 -0.5 -0.5 -0.5 -0.5  0.5 -0.5 -0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "   0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5  0.5 -0.5 -0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5  0.5\n",
      "  -0.5 -0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5\n",
      "   0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n",
      "  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlOGXivp/hD0FpYkG18AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f5652e7668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.image as mpimg\n",
    "read_pickle = pickle.load( open( \"BanglaLekha_mini/1.pickle\", \"rb\" ) )\n",
    "print (read_pickle.shape)\n",
    "print (read_pickle.size)\n",
    "#print (read_pickle)\n",
    "print (read_pickle[0])\n",
    "imgplot = plt.imshow(read_pickle[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " # shuffle images from each class to have random validation and training set then merge each classes into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:  (17900L, 28L, 28L) (17900L,)\n",
      "validation : (1000L, 28L, 28L) (1000L,)\n",
      "test : (1000L, 28L, 28L) (1000L,)\n"
     ]
    }
   ],
   "source": [
    "#print (train_dataset.shape)\n",
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset=np.ndarray((nb_rows, img_size , img_size), dtype=np.float32)\n",
    "        labels=np.ndarray(nb_rows,dtype=np.float32)\n",
    "    else:\n",
    "        dataset,labels=None, None\n",
    "    return dataset,labels\n",
    "def merge_datasets(pickle_files,train_size, valid_size=0):\n",
    "    num_classes= len(pickle_files)\n",
    "    valid_dataset, valid_labels= make_arrays(valid_size, image_size)#for validation \n",
    "    train_dataset, train_labels= make_arrays(train_size, image_size)\n",
    "    vsize_per_class= valid_size // num_classes\n",
    "    tsize_per_class= train_size // num_classes\n",
    "    \n",
    "    start_v,start_t =0 , 0\n",
    "    end_v, end_t =vsize_per_class , tsize_per_class\n",
    "    end_l=vsize_per_class+tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):#label=value or index of pickle files,example A,B,C,D\n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set=pickle.load(f)\n",
    "                #shuffle the letters to have random validation and training set\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter=letter_set[:vsize_per_class, : , :]\n",
    "                    valid_dataset[start_v:end_v, : , :] = valid_letter\n",
    "                    valid_labels[start_v:end_v]= label\n",
    "                    start_v +=vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "                    \n",
    "                train_letter= letter_set[vsize_per_class:end_l, :, :]\n",
    "                train_dataset[start_t:end_t, : ,:]=train_letter\n",
    "                train_labels[start_t:end_t]=label\n",
    "                start_t +=tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('unable to process data from', pickle_file, '.', e)\n",
    "            raise\n",
    "    return valid_dataset,valid_labels,train_dataset, train_labels\n",
    "\n",
    "train_size= 17900\n",
    "valid_size=1000\n",
    "test_size=1000\n",
    "\n",
    "valid_dataset, valid_labels,train_dataset,train_labels = merge_datasets(full_datasets, train_size , valid_size)\n",
    "test_dataset, test_labels,train_dataset,train_labels=merge_datasets(full_datasets,train_size,test_size)\n",
    "\n",
    "print('training: ' ,train_dataset.shape , train_labels.shape)\n",
    "print('validation :' , valid_dataset.shape , valid_labels.shape)\n",
    "print('test :', test_dataset.shape , test_labels.shape)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17900\n",
      "[11908 16022  6497 ...,  1326 16327  1818]\n",
      "1000\n",
      "[985 129 454 881 789 890 884 260 376 239 229 261 134 524 446 708 379 254\n",
      " 246 423 885 938 954 892 792 914 569 345 364 784 969 787 722 840 727 833\n",
      " 988 499 218 519 308 962  35  77 571 422 525 846 564 861 550 500 255 777\n",
      "  99 839 790 267 928 414 544 889 664 338 157  39 766 205 697 128 707 137\n",
      " 457 902 774 936  48  79 353 350 850 262 815 223 390 102 122  75 371 605\n",
      " 374 106 349 788 912 594 570 537 794 506 874 333 378 197 958 136 728 293\n",
      " 358 760 825  60  97 925 265  84 546 307 302 709 968 451 166 154 631 309\n",
      " 948 593 401 621  30 118 806 907 984 900 665 528 762 489 965 484 886 199\n",
      " 786 639 545 434 924 392 858 753 403 690 759 278 120 719 701 539 771 607\n",
      "  37  67 165  72 356 931 217 421 340  68 974 795 935 744 466 147 595 541\n",
      "  70 412 989 493 168 750 328 952 915 273 713 977  59  82 172 430 961 831\n",
      " 896 536 869 186 686 680 312 490 535 504 693 225 842 318 269 529 171 810\n",
      " 830 636  12 250 868 726 622 752 836  71 367 981 188 829 194 115 327 940\n",
      " 549   1 876  65  24 465 855 326 452 381 513 149 527 443 176 332 779 730\n",
      " 589 228  63 395 330 495 679 521 266 791 717 116 184 923 749 509 652 773\n",
      " 296  73 695  96 383 516 497  11  56 449 834 742 990 781 854 611 973 939\n",
      " 911 603 737 812 555 612 101 558 241 362 813 132 733   4 800 170 615 291\n",
      " 520 183 995 315 243 370 841 877 456 114 805 510 322 406 133 871 802   7\n",
      " 212 399 769 314 215 204 568 203 253 534 620 856 646 280 628 213 202 187\n",
      " 234 355 979 735 734 957 998 681 916 669 551 908 641 822  92 576 287 388\n",
      " 880 803 503 672 471  40 400 507 987 943 321 104 473 882 666 439 159 543\n",
      " 678 941 596  61 944  47 913  43 775 739 323 540 793 153 235 511 716 918\n",
      " 675 150 405 190 584 380 232 450  42  58 600 845 862 402 427 385  46 859\n",
      " 438 785 930 284 281 993 113 946 994 582  52 922 819 163 732 311 642 140\n",
      "  19 155 966 419 467 219  44 409  64  86 386 420 888 109 978 817  78 305\n",
      " 231 185 705 526 344 236 220 561 934 844 554 492   2 208 553 618 351 823\n",
      " 562 654 865  38 633 180 481 193 765 480 103 682 361 486 258 299 346 224\n",
      " 581 746 169  20 767 820 316 688 843 359 919 574 508 445 164 929 295 411\n",
      " 158 901  90 625 216 247 579 271 864 586 458 238 485 597 110 857 285 563\n",
      " 515  88 325 776 337 801 560 111  76 469 538 718 917 783 201  10 195 160\n",
      " 447 692 661 632 658 277 152 720 487 909 437 282 699 951 460  27 227 878\n",
      " 230 112  29 442 248 591 498 838 624 522 363 221 673 895 937 599 319 945\n",
      " 352 975 660 365 547  54 391 196 518  80 906  83 837 947 270 289 264 566\n",
      " 764 482 300 782 920 233 206 121 921 653 926 577 590 432 141  53 237  22\n",
      " 811  17 310 162 329 468 619 138 249 933 677 778 662 394 740  28 970 478\n",
      " 700 971 552 684 119 470 592 763 397 341 780 655 670 556 667 715  81 125\n",
      " 630 598  33 992 580 706  55 671   8 649 189 723  45 502 398 435 117 286\n",
      "  36 453 724 691 105 848 377 336 757 572 745 676 799 334 393 369 501 251\n",
      " 455  26 366 290 575 949  87 461  85 635 712 872 244 770 209 263 211 626\n",
      "  14 964 426 982 182 494 731 177 240 583 627 416 567 483 879 980 404 303\n",
      " 143  91 808 647 847 967 747 807 279 610 488 491 659 384 306 142 853 601\n",
      " 651 413 410 144 972 725 893 342 804 276 433 259 373 875 252 441   3 123\n",
      " 942 146 798 617 816 173 459 650 657 756 505  25   9 703 512 108 694  23\n",
      " 827 496 283 743 996  98 372 156 851  89 448 517 124 826 292 606 425 863\n",
      " 222 135 754  18 179 772  62 475 192 320 614 174  57 275 200 689 983 644\n",
      " 357 408 883 585 272 343 860 891 683 729  49 685 959  51 955 748  50 643\n",
      " 256  34 530  41 588 578 148 768 479   0 648 464  69 268 818  31 950 417\n",
      " 704 755 324 131 472 953  74  95 382 721 396 348  94 609 674 436 514 898\n",
      " 645 474 640 638 637 191 573 623 387 738 687 429 999 463 178 151 304 887\n",
      " 761 139 976 960 207 145 656  32 904 963  15 867 796 175 418 274 347 532\n",
      " 956  66 339 431 608 698 317 257 181 542 629 100 127 991 849 832 444 710\n",
      " 616 899 835 294 932 559 523 986 797 335 198 758 297 354 613 389 440 375\n",
      " 741   6 548 696 905 245 167 873 714 126 910 210 702  16 214 894 997 814\n",
      " 565 711 852 428 107  93 531 663 870 415 407 557 226 331 301 462 927 242\n",
      " 298 602 828 533 604 130 368 897 751 587 634 821 903 476  13 477 824 424\n",
      " 809 161 668 866 736 360 288  21   5 313]\n",
      "1000\n",
      "[511 514 100 942  54 299 663 753 363 690 520 236 673 631 492 228 863  28\n",
      " 902 242 436 824 248 370 970 556 765 122  14 482 819 406   8 153  70 754\n",
      " 792 387 231 247 200 307 866 470 501 874 595 486 958 385 671 639 336 700\n",
      " 723 167 273 573 473 596 807 850 666 906 617  99 760 533 504 334 254 710\n",
      "  22 290 357 530 980   2 118  71 563 822 185 246 702 538 709 629 718 451\n",
      " 449 927  27 893 128 605 325  78 937 986 169  91 321 543 804 668 833 401\n",
      " 875 696 829 120 259 397 894 367 212 173 975 278 740 182 641 869 987 461\n",
      " 308 562 647 925 678 294 485 616 350   5 582 141 230 731 440 613 976 149\n",
      " 569 487 447 651 257 873 178 661 469 365 327 677 490 276 778 907 477 480\n",
      " 837 147 659 780 146  65 997 450 983 974 855 269 176 233 674  88  43 681\n",
      " 862 684 152 749 491 643 654 784 636 479 183 489 437 235 909 322 413 708\n",
      " 878 519 939  86 524 625 457 764 660 374 888 263 739 808  84 699 725 320\n",
      " 463 703 353 773 390 571 908 954 581 931 867  41 904 539 280  80 886 565\n",
      " 719 499 697 900  83 923 349 559 854  38 227  76 658 382 106 847 226 777\n",
      " 422  58 238 809 946 157 794 984  23 554 813 864 244 333 379 624 292 550\n",
      " 502 811 979 172 848 528 144 467 614 279 411 896 694 298 420 386 398  40\n",
      " 679  55  68 645 814 609  61 362 270 191  47 601 301  93 380 947 924 537\n",
      " 570 466 743 110 471 525 615 345 135 845 701 755  46  92  57 359 111 726\n",
      " 190 960 291 303 287 415  85 351 761 916 432 579 589 825  17 115 978 285\n",
      " 592 826 318 830 544 956 846  63 941 203 426 594 441 435   0 313 206 591\n",
      " 324 388 688 889 786 265 781 459 529 249 593 771 289 683 500   9 137 121\n",
      " 129 583  95  18 310 959 462 483 587 328 775 428 402 912 277 851 676 575\n",
      " 275 812 114 800  66 475 897 465 512 356  64  31 932 938 680 113 498 546\n",
      " 342 729  45 934 757 999 150 779 165 759 205 392 689 628  52  94 309 124\n",
      " 127  24  73 564 305 434 730 992 831 123 496 103  29 612 532  81 168 239\n",
      " 716 860 842 427 474 404 926 421  16 495 834 240 670 516 476 580 399 704\n",
      " 383 928 713 560 542 384 507  67 188 698 872 409 998 655 315 506 572  44\n",
      " 267 505 795 964 798 186 535 968 774 650 460 687 391 369 586 853  75 722\n",
      " 274 967 652  20 250 552 407 338 225 405 164 361 302 787 478 982 373 377\n",
      " 852 783 509 300 789 198 898 817 425 180  89 692 882 281 284 297 944 973\n",
      " 914 827 395 343 202 561 472 635 935 574  36 836 905 917 669 772 540 919\n",
      "  60 534  39 207 791 793 446 557  51 429  21 223 215 856 430  50 901 711\n",
      " 255 748 820 243 640 452 768 358 885 918 232 256 347 260 712 134 576 752\n",
      " 828 952 667 742 261 767 584 151 237 859 312 598 196 316 672 283 994 314\n",
      " 262 335  69 868 566 116 253 132 348 884 266 252 408 138 130 957 296 154\n",
      " 396 218  15 510 608 258 763 194 412   3 745 801 197 295 389  87 637 222\n",
      " 693 642 306  33 339 410 162 171 737 414 818 193 567 796 626 985 649 805\n",
      " 454 865  79 376 517 656 210 899 972 161 913 155 879 632 802  34 911 963\n",
      " 214 170 933 736 870  26 945 330 762 990   1 971  90 424 503  59 245 621\n",
      " 464  42 630  49 468 211 541 662 943 264 326 174 714 216 682 251 806 741\n",
      " 219 403 344 648 177 139 126 201 522 394 523 112 724 915 766 125 221 366\n",
      " 600 442 548 597 319 955 965 317 417 604 577 368 143 104 705 993 840 513\n",
      " 268 497 785 481 756  62 271 109 929  10 735 331 352 921 438 288 453 547\n",
      " 815  74 229 208 930  98 966 199 776 728 508  77 769 107 187 455 903 444\n",
      " 607 746 989 119 618 578 184 448 988 337 195 224 715   7 433 602 158 951\n",
      " 857 858 418 922 895 304 620 355 733 653  56 493 488 423 220 456 996 920\n",
      " 717 272 340 751 835   4 810 940   6 293 839 949  30 568 799 494 599 140\n",
      " 891 378 458 346 286 619 881 189 166 695 549 400 883  37  19 364 526  12\n",
      " 961 360 981 646 282 213 148 887  72 431 142 160 758 691 707  35 634 545\n",
      " 515 181 871 108 747  82 159 721 419 720 782 738 734  25 175 797 936 686\n",
      " 910 877 145 603 527 790 558 234 585 948 969 744 606 991  11 381 788 341\n",
      " 117 610 841 657 838 531 102 484  53 443 638 995 627  13 588 803 685 105\n",
      " 890 665  97 892 416 521 536 611 518 816 633 551 555 156 750 770 553 375\n",
      " 843 950 133 876 880 204 323 311 823 179 861 732 217 849 727 821 590  32\n",
      " 241 329 962 445 623 354  96 622 131 372 644 136 664 675 706 439 844 393\n",
      " 209 832 163 371  48 953 977 332 101 192]\n"
     ]
    }
   ],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    print (labels.shape[0])\n",
    "    print (permutation)\n",
    "    shuffled_dataset= dataset[permutation, : , :]\n",
    "    shuffled_labels= labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "train_dataset , train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset , test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset , valid_labels = randomize(valid_dataset, valid_labels)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwy8VZfilogy/VNT/A3GOMeO6YRjKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9c3d8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgplot = plt.imshow(train_dataset [0])\n",
    "print (train_labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'banglaLekha_mini.pickle')\n",
    "try:\n",
    "    f = open(pickle_file , 'wb')\n",
    "    save= {\n",
    "        'train_dataset' : train_dataset,\n",
    "        'train_labels'  : train_labels,\n",
    "        'valid_dataset' : valid_dataset,\n",
    "        'valid_labels'  : valid_labels,\n",
    "        'test_dataset'  : test_dataset,\n",
    "        'test_labels'    : test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f , pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('unable to save data to', pickle_file , ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressed pickcle size: 62486414\n"
     ]
    }
   ],
   "source": [
    "statinfo= os.stat(pickle_file)\n",
    "print('compressed pickcle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "\n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [ 0.5  0.5  0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [ 0.5  0.5  0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ...,  0.5  0.5  0.5]\n",
      "  [-0.5 -0.5 -0.5 ...,  0.5  0.5  0.5]\n",
      "  [-0.5 -0.5 -0.5 ...,  0.5  0.5  0.5]]\n",
      "\n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "\n",
      " ..., \n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "\n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "\n",
      " [[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  ..., \n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      "  [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]]\n",
      "Time: 0.16s\n",
      "valid -> train overlap: 899 samples\n",
      "test  -> train overlap: 3 samples\n",
      "test  -> valid overlap: 60 samples\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import hashlib\n",
    "\n",
    "f=open(pickle_file, 'rb')\n",
    "read_pickle = pickle.load( f )\n",
    "train_dataset=read_pickle['train_dataset']\n",
    "print(train_dataset)\n",
    "train_labels=read_pickle['train_labels']\n",
    "valid_dataset=read_pickle['valid_dataset']\n",
    "valid_labels=read_pickle['valid_labels']\n",
    "test_dataset=read_pickle['test_dataset']\n",
    "test_labels=read_pickle['test_labels']\n",
    "\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "train_hashes = [hashlib.sha1(x).digest() for x in train_dataset]\n",
    "valid_hashes = [hashlib.sha1(x).digest() for x in valid_dataset]\n",
    "test_hashes  = [hashlib.sha1(x).digest() for x in test_dataset]\n",
    "\n",
    "valid_in_train = np.in1d(valid_hashes, train_hashes)\n",
    "test_in_train  = np.in1d(test_hashes,  train_hashes)\n",
    "test_in_valid  = np.in1d(test_hashes,  valid_hashes)\n",
    "\n",
    "valid_keep = ~valid_in_train\n",
    "test_keep  = ~(test_in_train | test_in_valid)\n",
    "\n",
    "valid_dataset_clean = valid_dataset[valid_keep]\n",
    "valid_labels_clean  = valid_labels [valid_keep]\n",
    "\n",
    "test_dataset_clean = test_dataset[test_keep]\n",
    "test_labels_clean  = test_labels [test_keep]\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Time: %0.2fs\" % (t2 - t1))\n",
    "print(\"valid -> train overlap: %d samples\" % valid_in_train.sum())\n",
    "print(\"test  -> train overlap: %d samples\" % test_in_train.sum())\n",
    "print(\"test  -> valid overlap: %d samples\" % test_in_valid.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937L, 28L, 28L)\n",
      "734608\n",
      "(101L, 28L, 28L)\n",
      "79184\n",
      "(17900L, 28L, 28L)\n",
      "14033600\n"
     ]
    }
   ],
   "source": [
    "print (test_dataset_clean.shape)\n",
    "print (test_dataset_clean.size)\n",
    "print (valid_dataset_clean.shape)\n",
    "print (valid_dataset_clean.size)\n",
    "print (train_dataset.shape)\n",
    "print (train_dataset.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving cleaned Dataset as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'banglaLekha_mini_clean.pickle')\n",
    "try:\n",
    "    f = open(pickle_file , 'wb')\n",
    "    save= {\n",
    "        'train_dataset' : train_dataset,\n",
    "        'train_labels'  : train_labels,\n",
    "        'valid_dataset_clean' : valid_dataset,\n",
    "        'valid_labels'  : valid_labels,\n",
    "        'test_dataset_clean'  : test_dataset,\n",
    "        'test_labels'    : test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f , pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('unable to save data to', pickle_file , ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressed pickcle size: 62486426\n"
     ]
    }
   ],
   "source": [
    "statinfo= os.stat(pickle_file)\n",
    "print('compressed pickcle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
